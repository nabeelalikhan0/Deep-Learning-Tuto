{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea074755",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  What is an **RNN**?  (Recurrent Neural Network)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘¶ Baby Definition:\n",
    "\n",
    "> A Recurrent Neural Network (RNN) is a type of neural network that can **remember things from the past**.\n",
    "\n",
    "Itâ€™s like a little brain that, when reading something, says:\n",
    "\n",
    "> â€œHey! I remember what I just read... and Iâ€™ll use that to guess what comes next.â€\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“– Example:\n",
    "\n",
    "You read this sentence:\n",
    "\n",
    "> â€œRoses are red, violets areâ€¦â€\n",
    "\n",
    "A regular neural network would treat each word as **separate**.\n",
    "\n",
    "But an RNN says:\n",
    "\n",
    "> â€œOh! I remember â€˜roses are redâ€™, so the next word is probably â€˜blueâ€™ or â€˜purpleâ€™!â€\n",
    "\n",
    "âœ… RNN uses **memory of past inputs** to predict the future.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ” Why Use RNNs?\n",
    "\n",
    "RNNs are perfect for **sequence data**, like:\n",
    "\n",
    "| Type         | Examples                        |\n",
    "| ------------ | ------------------------------- |\n",
    "| Text         | Sentences, chats, documents     |\n",
    "| Time Series  | Stock prices, weather, ECG, IoT |\n",
    "| Audio        | Speech, music                   |\n",
    "| Video Frames | Step-by-step motion             |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”„ How RNN Works\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§± Regular Neural Network:\n",
    "\n",
    "It works like this:\n",
    "\n",
    "```\n",
    "Input â†’ Hidden â†’ Output\n",
    "```\n",
    "\n",
    "Every input is treated **independently**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” RNN:\n",
    "\n",
    "It loops back to itself! Like this:\n",
    "\n",
    "```\n",
    "Inputâ‚œ â†’ Hiddenâ‚œ â†’ Outputâ‚œ  \n",
    "          â†‘  \n",
    "       Hiddenâ‚œâ‚‹â‚\n",
    "```\n",
    "\n",
    "At **every time step `t`**, it:\n",
    "\n",
    "* Takes **inputâ‚œ** (e.g., a word, number)\n",
    "* Combines it with the **previous hidden state** (i.e., memory)\n",
    "* Produces an output AND updates the memory\n",
    "\n",
    "This lets it **remember what came before**!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘ï¸ Visual Summary\n",
    "\n",
    "```\n",
    "Time Steps:     tâ‚     â†’     tâ‚‚     â†’     tâ‚ƒ\n",
    "\n",
    "Inputs:        â€œIâ€     â€œamâ€        â€œNabeelâ€\n",
    "\n",
    "Outputs:       [ ]     [ ]          [Hello!]\n",
    "\n",
    "Memory:       hâ‚€ â†’    hâ‚ â†’         hâ‚‚ â†’ ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¥ Simple Code Example (Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  Whatâ€™s Inside the RNN Cell?\n",
    "\n",
    "Each cell does this at time `t`:\n",
    "\n",
    "```\n",
    "hâ‚œ = tanh(Wx * xâ‚œ + Wh * hâ‚œâ‚‹â‚ + b)\n",
    "```\n",
    "\n",
    "| Symbol | Meaning                     |\n",
    "| ------ | --------------------------- |\n",
    "| `xâ‚œ`   | input at time t             |\n",
    "| `hâ‚œâ‚‹â‚` | previous hidden state       |\n",
    "| `Wx`   | weight for input            |\n",
    "| `Wh`   | weight for previous hidden  |\n",
    "| `b`    | bias                        |\n",
    "| `tanh` | activation (like squashing) |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ˜• But Thereâ€™s a Problemâ€¦\n",
    "\n",
    "### âŒ Standard RNNs **struggle with long-term memory**:\n",
    "\n",
    "* When the input sequence is long, it forgets what happened early\n",
    "* This is called the **vanishing gradient problem**\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš€ Solution: LSTM and GRU\n",
    "\n",
    "| Type     | What It Does                                         |\n",
    "| -------- | ---------------------------------------------------- |\n",
    "| **LSTM** | Long Short-Term Memory: remembers longer, uses gates |\n",
    "| **GRU**  | Gated Recurrent Unit: simpler, also remembers better |\n",
    "\n",
    "These are improved versions of RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  TL;DR Summary\n",
    "\n",
    "| Feature      | Description                            |\n",
    "| ------------ | -------------------------------------- |\n",
    "| RNN          | Remembers past inputs in sequences     |\n",
    "| Good For     | Text, time-series, speech, music       |\n",
    "| Architecture | Loops over time steps, shares memory   |\n",
    "| Problem      | Forgets long sequences (fixed by LSTM) |\n",
    "| Variants     | LSTM, GRU                              |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“š Real-Life Analogy\n",
    "\n",
    "Imagine youâ€™re **listening to a story**. You need to remember earlier parts to understand whatâ€™s going on later.\n",
    "\n",
    "> **RNN is a brain that listens step-by-step and keeps notes in its memory.** ğŸ§ ğŸ“\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa7159",
   "metadata": {},
   "source": [
    "\n",
    "> ğŸ” **LSTM (Long Short-Term Memory)**\n",
    "> âš™ï¸ **GRU (Gated Recurrent Unit)**\n",
    "\n",
    "These are special types of RNNs that **solve the forgetting problem** of normal RNNs.\n",
    "\n",
    "Letâ€™s go ultra-simple again:\n",
    "\n",
    "> ğŸ‘¶ Explain like youâ€™re 5\n",
    "> âš›ï¸ Define every single part (no atom left behind)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  Why LSTM and GRU Were Created?\n",
    "\n",
    "---\n",
    "\n",
    "## âŒ Problem with Basic RNN\n",
    "\n",
    "* Regular RNNs **forget long-term information**.\n",
    "* Like someone who remembers only the last sentence in a story.\n",
    "\n",
    "This is called the **vanishing gradient problem** â€” the **memory fades** the further back you go.\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… Enter: **LSTM** & **GRU**\n",
    "\n",
    "> They use **â€œgatesâ€** to **decide what to remember** and **what to forget**.\n",
    "> These are like **smart memory filters** ğŸ”“ğŸ”\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§¬ Let's Start with LSTM\n",
    "\n",
    "**Long Short-Term Memory**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘¶ Analogy:\n",
    "\n",
    "Imagine your brain is taking notes ğŸ“ while listening to a story.\n",
    "But you donâ€™t want to write down **everything**, just the **important stuff**.\n",
    "\n",
    "So you ask yourself:\n",
    "\n",
    "1. What should I forget? âŒ\n",
    "2. What should I remember? âœ…\n",
    "3. What should I write down now? âœï¸\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ LSTM Architecture (Block View)\n",
    "\n",
    "Each LSTM cell has:\n",
    "\n",
    "| Gate            | What It Does                            | Emoji |\n",
    "| --------------- | --------------------------------------- | ----- |\n",
    "| **Forget Gate** | Decides what to forget from past memory | ğŸ§¹    |\n",
    "| **Input Gate**  | Decides what new info to add to memory  | â•     |\n",
    "| **Output Gate** | Decides what part of memory to output   | ğŸ“¤    |\n",
    "| **Cell State**  | The main memory line                    | ğŸ§     |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ LSTM Flow:\n",
    "\n",
    "```text\n",
    "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    " Previous Cell â†’â”‚  LSTM Cell   â”‚â†’ New Cell State\n",
    "State & Output  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â†‘              â†‘\n",
    "        Input        Previous Output\n",
    "```\n",
    "\n",
    "The gates inside the LSTM decide:\n",
    "\n",
    "* \"Forget old junk\"\n",
    "* \"Add useful new stuff\"\n",
    "* \"Output the right memory at this time\"\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”£ What LSTM Learns:\n",
    "\n",
    "```python\n",
    "c_t = (forget * c_t-1) + (input * new_info)\n",
    "h_t = output_gate * tanh(c_t)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Real-Life Example:\n",
    "\n",
    "You're reading a novel.\n",
    "\n",
    "* You **forget** the color of the characterâ€™s shoes (not important) ğŸ§¹\n",
    "* You **remember** the killerâ€™s name (very important) ğŸ§ \n",
    "* You **talk about** the twist at the end (output) ğŸ“¤\n",
    "\n",
    "---\n",
    "\n",
    "# âš™ï¸ Now, GRU â€” Gated Recurrent Unit\n",
    "\n",
    "## ğŸ‘¶ Even Simpler Analogy:\n",
    "\n",
    "GRU says:\n",
    "\n",
    "> â€œWhy have 3 gates like LSTM? Letâ€™s make it simpler with just **2 gates**.â€\n",
    "\n",
    "| GRU Gate        | What It Does                     | Emoji |\n",
    "| --------------- | -------------------------------- | ----- |\n",
    "| **Update Gate** | Like input + forget combined     | ğŸ”„    |\n",
    "| **Reset Gate**  | Controls how much past to forget | ğŸ§¹    |\n",
    "\n",
    "It **merges the forget and input gates** into **1 update gate** = faster and simpler!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”£ GRU Equations:\n",
    "\n",
    "```python\n",
    "h_t = (1 - update) * old_memory + update * new_memory\n",
    "```\n",
    "\n",
    "### âœ… Fewer parameters = faster training\n",
    "\n",
    "### âŒ Slightly less flexible than LSTM in some cases\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¥Š LSTM vs GRU â€” Side-by-Side\n",
    "\n",
    "| Feature        | LSTM                       | GRU                       |\n",
    "| -------------- | -------------------------- | ------------------------- |\n",
    "| Gates          | 3 (input, forget, output)  | 2 (update, reset)         |\n",
    "| Cell State     | Yes (separate from output) | No (combined with hidden) |\n",
    "| Complexity     | More complex               | Simpler                   |\n",
    "| Performance    | Better for long sequences  | Faster and still powerful |\n",
    "| Training Speed | Slower                     | Faster                    |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§ª Code Examples (Keras)\n",
    "\n",
    "### ğŸ” LSTM\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "### âš™ï¸ GRU\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(64, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  TL;DR Summary\n",
    "\n",
    "| Concept     | LSTM                                | GRU                       |\n",
    "| ----------- | ----------------------------------- | ------------------------- |\n",
    "| Memory Type | Has separate **cell state** ğŸ§       | Uses hidden state only    |\n",
    "| Gates       | 3 gates (input, forget, output)     | 2 gates (update, reset)   |\n",
    "| Size        | Bigger, more accurate on long tasks | Smaller, faster to train  |\n",
    "| Use Case    | Language models, translation        | Real-time chatbots, music |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“ Final Analogy:\n",
    "\n",
    "| Model | Like...                                              |\n",
    "| ----- | ---------------------------------------------------- |\n",
    "| RNN   | A person with **short-term memory** ğŸ§â€â™‚ï¸            |\n",
    "| LSTM  | A person taking **notes with a smart notebook** ğŸ““ğŸ§  |\n",
    "| GRU   | A person with a **simple but fast brain** ğŸ§ âš¡        |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3bac4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  What is GRU? Gated Recurrent Unit\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘¶ Simple Definition:\n",
    "\n",
    "> GRU is a type of Recurrent Neural Network (RNN) that has a **smart memory system** built with just **two gates** â€” not three like LSTM.\n",
    "\n",
    "It's like a little brain that:\n",
    "\n",
    "* Remembers whatâ€™s important ğŸ”\n",
    "* Forgets whatâ€™s not ğŸ§¹\n",
    "\n",
    "And it does this faster than LSTM!\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¤” Why Was GRU Created?\n",
    "\n",
    "* LSTM was great but **a bit heavy and slow** ğŸ¢\n",
    "* So GRU was designed to:\n",
    "\n",
    "  * Be **simpler** (fewer gates)\n",
    "  * Be **faster to train**\n",
    "  * Perform **almost as well** (or better in some tasks)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§± GRU Structure\n",
    "\n",
    "GRU has **2 gates** instead of 3:\n",
    "\n",
    "| Gate                | What It Does                           | Emoji |\n",
    "| ------------------- | -------------------------------------- | ----- |\n",
    "| **Update Gate** `z` | Decides **how much past info to keep** | ğŸ”„    |\n",
    "| **Reset Gate** `r`  | Decides **how much past to forget**    | ğŸ§¹    |\n",
    "\n",
    "That's it. No output gate, no separate cell state â€” it's all handled in **1 hidden state**.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ How GRU Works â€” Step by Step\n",
    "\n",
    "At each time step:\n",
    "\n",
    "1. ğŸ§¹ **Reset Gate (`r`)**:\n",
    "\n",
    "   * â€œShould I forget the old memory?â€\n",
    "\n",
    "2. ğŸ”„ **Update Gate (`z`)**:\n",
    "\n",
    "   * â€œShould I keep old memory or update with new info?â€\n",
    "\n",
    "3. ğŸ§  **New Hidden State (`hâ‚œ`)**:\n",
    "\n",
    "   * Combines the old and new info based on the gates' decisions\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ GRU Equations (Explained Simply)\n",
    "\n",
    "Letâ€™s break it down:\n",
    "\n",
    "```python\n",
    "zâ‚œ = Ïƒ(Wz Â· xâ‚œ + Uz Â· hâ‚œâ‚‹â‚)     # Update gate\n",
    "râ‚œ = Ïƒ(Wr Â· xâ‚œ + Ur Â· hâ‚œâ‚‹â‚)     # Reset gate\n",
    "\n",
    "hÌƒâ‚œ = tanh(W Â· xâ‚œ + U Â· (râ‚œ * hâ‚œâ‚‹â‚))  # New memory\n",
    "\n",
    "hâ‚œ = (1 - zâ‚œ) * hâ‚œâ‚‹â‚ + zâ‚œ * hÌƒâ‚œ       # Final output (mix old + new)\n",
    "```\n",
    "\n",
    "| Term   | Meaning                                       |\n",
    "| ------ | --------------------------------------------- |\n",
    "| `Ïƒ`    | Sigmoid activation (squashes between 0 and 1) |\n",
    "| `tanh` | Activation function for output                |\n",
    "| `xâ‚œ`   | Input at time t                               |\n",
    "| `hâ‚œâ‚‹â‚` | Previous hidden state                         |\n",
    "| `hÌƒâ‚œ`  | Candidate new hidden state                    |\n",
    "| `hâ‚œ`   | Final hidden state/output at time t           |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Intuition:\n",
    "\n",
    "GRU says:\n",
    "\n",
    "> â€œLet me **control memory** using just 2 knobs â€” update and reset â€” and Iâ€™ll do the same job as LSTM faster!â€\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ’» GRU Code (in Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(64, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“Š GRU vs LSTM\n",
    "\n",
    "| Feature       | **GRU**                       | **LSTM**                          |\n",
    "| ------------- | ----------------------------- | --------------------------------- |\n",
    "| Gates         | 2 (update, reset)             | 3 (input, forget, output)         |\n",
    "| Simpler?      | âœ… Yes                         | âŒ More complex                    |\n",
    "| Memory        | Single hidden state           | Hidden + cell state               |\n",
    "| Speed         | âš¡ Faster                      | ğŸ¢ Slower                         |\n",
    "| Performance   | Good in short & mid sequences | Better in long, complex sequences |\n",
    "| Training Time | Less                          | More                              |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ’¬ Real-Life Analogy\n",
    "\n",
    "Imagine a student reading a story.\n",
    "\n",
    "* GRU is a **fast learner**:\n",
    "\n",
    "  * Remembers important parts\n",
    "  * Erases whatâ€™s not needed\n",
    "  * All using **2 switches (update & reset)**\n",
    "\n",
    "Whereas LSTM is like:\n",
    "\n",
    "* A **note-taking overachiever** with 3 tools (input, forget, output)\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ” When to Use GRU?\n",
    "\n",
    "| Use Case                    | GRU Works Well? |\n",
    "| --------------------------- | --------------- |\n",
    "| Real-time prediction        | âœ… (fast)        |\n",
    "| Text classification         | âœ…               |\n",
    "| Speech/audio processing     | âœ…               |\n",
    "| Time series forecasting     | âœ…               |\n",
    "| Very long-term memory tasks | âŒ Use LSTM      |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  TL;DR Summary\n",
    "\n",
    "| Term    | Definition                                     |\n",
    "| ------- | ---------------------------------------------- |\n",
    "| GRU     | Gated Recurrent Unit â€“ a type of RNN           |\n",
    "| Goal    | Keep important memory, forget unimportant ones |\n",
    "| Gates   | Update gate ğŸ”„, Reset gate ğŸ§¹                  |\n",
    "| Simpler | Yes, faster than LSTM                          |\n",
    "| Used in | NLP, time series, real-time tasks              |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
