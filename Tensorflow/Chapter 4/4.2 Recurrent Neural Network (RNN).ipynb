{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea074755",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🧠 What is an **RNN**?  (Recurrent Neural Network)\n",
    "\n",
    "---\n",
    "\n",
    "## 👶 Baby Definition:\n",
    "\n",
    "> A Recurrent Neural Network (RNN) is a type of neural network that can **remember things from the past**.\n",
    "\n",
    "It’s like a little brain that, when reading something, says:\n",
    "\n",
    "> “Hey! I remember what I just read... and I’ll use that to guess what comes next.”\n",
    "\n",
    "---\n",
    "\n",
    "### 📖 Example:\n",
    "\n",
    "You read this sentence:\n",
    "\n",
    "> “Roses are red, violets are…”\n",
    "\n",
    "A regular neural network would treat each word as **separate**.\n",
    "\n",
    "But an RNN says:\n",
    "\n",
    "> “Oh! I remember ‘roses are red’, so the next word is probably ‘blue’ or ‘purple’!”\n",
    "\n",
    "✅ RNN uses **memory of past inputs** to predict the future.\n",
    "\n",
    "---\n",
    "\n",
    "# 🔁 Why Use RNNs?\n",
    "\n",
    "RNNs are perfect for **sequence data**, like:\n",
    "\n",
    "| Type         | Examples                        |\n",
    "| ------------ | ------------------------------- |\n",
    "| Text         | Sentences, chats, documents     |\n",
    "| Time Series  | Stock prices, weather, ECG, IoT |\n",
    "| Audio        | Speech, music                   |\n",
    "| Video Frames | Step-by-step motion             |\n",
    "\n",
    "---\n",
    "\n",
    "# 🔄 How RNN Works\n",
    "\n",
    "---\n",
    "\n",
    "## 🧱 Regular Neural Network:\n",
    "\n",
    "It works like this:\n",
    "\n",
    "```\n",
    "Input → Hidden → Output\n",
    "```\n",
    "\n",
    "Every input is treated **independently**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 RNN:\n",
    "\n",
    "It loops back to itself! Like this:\n",
    "\n",
    "```\n",
    "Inputₜ → Hiddenₜ → Outputₜ  \n",
    "          ↑  \n",
    "       Hiddenₜ₋₁\n",
    "```\n",
    "\n",
    "At **every time step `t`**, it:\n",
    "\n",
    "* Takes **inputₜ** (e.g., a word, number)\n",
    "* Combines it with the **previous hidden state** (i.e., memory)\n",
    "* Produces an output AND updates the memory\n",
    "\n",
    "This lets it **remember what came before**!\n",
    "\n",
    "---\n",
    "\n",
    "## 👁️ Visual Summary\n",
    "\n",
    "```\n",
    "Time Steps:     t₁     →     t₂     →     t₃\n",
    "\n",
    "Inputs:        “I”     “am”        “Nabeel”\n",
    "\n",
    "Outputs:       [ ]     [ ]          [Hello!]\n",
    "\n",
    "Memory:       h₀ →    h₁ →         h₂ → ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🔥 Simple Code Example (Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 What’s Inside the RNN Cell?\n",
    "\n",
    "Each cell does this at time `t`:\n",
    "\n",
    "```\n",
    "hₜ = tanh(Wx * xₜ + Wh * hₜ₋₁ + b)\n",
    "```\n",
    "\n",
    "| Symbol | Meaning                     |\n",
    "| ------ | --------------------------- |\n",
    "| `xₜ`   | input at time t             |\n",
    "| `hₜ₋₁` | previous hidden state       |\n",
    "| `Wx`   | weight for input            |\n",
    "| `Wh`   | weight for previous hidden  |\n",
    "| `b`    | bias                        |\n",
    "| `tanh` | activation (like squashing) |\n",
    "\n",
    "---\n",
    "\n",
    "# 😕 But There’s a Problem…\n",
    "\n",
    "### ❌ Standard RNNs **struggle with long-term memory**:\n",
    "\n",
    "* When the input sequence is long, it forgets what happened early\n",
    "* This is called the **vanishing gradient problem**\n",
    "\n",
    "---\n",
    "\n",
    "# 🚀 Solution: LSTM and GRU\n",
    "\n",
    "| Type     | What It Does                                         |\n",
    "| -------- | ---------------------------------------------------- |\n",
    "| **LSTM** | Long Short-Term Memory: remembers longer, uses gates |\n",
    "| **GRU**  | Gated Recurrent Unit: simpler, also remembers better |\n",
    "\n",
    "These are improved versions of RNNs.\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 TL;DR Summary\n",
    "\n",
    "| Feature      | Description                            |\n",
    "| ------------ | -------------------------------------- |\n",
    "| RNN          | Remembers past inputs in sequences     |\n",
    "| Good For     | Text, time-series, speech, music       |\n",
    "| Architecture | Loops over time steps, shares memory   |\n",
    "| Problem      | Forgets long sequences (fixed by LSTM) |\n",
    "| Variants     | LSTM, GRU                              |\n",
    "\n",
    "---\n",
    "\n",
    "# 📚 Real-Life Analogy\n",
    "\n",
    "Imagine you’re **listening to a story**. You need to remember earlier parts to understand what’s going on later.\n",
    "\n",
    "> **RNN is a brain that listens step-by-step and keeps notes in its memory.** 🧠📝\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa7159",
   "metadata": {},
   "source": [
    "\n",
    "> 🔐 **LSTM (Long Short-Term Memory)**\n",
    "> ⚙️ **GRU (Gated Recurrent Unit)**\n",
    "\n",
    "These are special types of RNNs that **solve the forgetting problem** of normal RNNs.\n",
    "\n",
    "Let’s go ultra-simple again:\n",
    "\n",
    "> 👶 Explain like you’re 5\n",
    "> ⚛️ Define every single part (no atom left behind)\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 Why LSTM and GRU Were Created?\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ Problem with Basic RNN\n",
    "\n",
    "* Regular RNNs **forget long-term information**.\n",
    "* Like someone who remembers only the last sentence in a story.\n",
    "\n",
    "This is called the **vanishing gradient problem** — the **memory fades** the further back you go.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Enter: **LSTM** & **GRU**\n",
    "\n",
    "> They use **“gates”** to **decide what to remember** and **what to forget**.\n",
    "> These are like **smart memory filters** 🔓🔐\n",
    "\n",
    "---\n",
    "\n",
    "# 🧬 Let's Start with LSTM\n",
    "\n",
    "**Long Short-Term Memory**\n",
    "\n",
    "---\n",
    "\n",
    "## 👶 Analogy:\n",
    "\n",
    "Imagine your brain is taking notes 📝 while listening to a story.\n",
    "But you don’t want to write down **everything**, just the **important stuff**.\n",
    "\n",
    "So you ask yourself:\n",
    "\n",
    "1. What should I forget? ❌\n",
    "2. What should I remember? ✅\n",
    "3. What should I write down now? ✍️\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 LSTM Architecture (Block View)\n",
    "\n",
    "Each LSTM cell has:\n",
    "\n",
    "| Gate            | What It Does                            | Emoji |\n",
    "| --------------- | --------------------------------------- | ----- |\n",
    "| **Forget Gate** | Decides what to forget from past memory | 🧹    |\n",
    "| **Input Gate**  | Decides what new info to add to memory  | ➕     |\n",
    "| **Output Gate** | Decides what part of memory to output   | 📤    |\n",
    "| **Cell State**  | The main memory line                    | 🧠    |\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 LSTM Flow:\n",
    "\n",
    "```text\n",
    "                ┌──────────────┐\n",
    " Previous Cell →│  LSTM Cell   │→ New Cell State\n",
    "State & Output  └──────────────┘\n",
    "           ↑              ↑\n",
    "        Input        Previous Output\n",
    "```\n",
    "\n",
    "The gates inside the LSTM decide:\n",
    "\n",
    "* \"Forget old junk\"\n",
    "* \"Add useful new stuff\"\n",
    "* \"Output the right memory at this time\"\n",
    "\n",
    "---\n",
    "\n",
    "## 🔣 What LSTM Learns:\n",
    "\n",
    "```python\n",
    "c_t = (forget * c_t-1) + (input * new_info)\n",
    "h_t = output_gate * tanh(c_t)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Real-Life Example:\n",
    "\n",
    "You're reading a novel.\n",
    "\n",
    "* You **forget** the color of the character’s shoes (not important) 🧹\n",
    "* You **remember** the killer’s name (very important) 🧠\n",
    "* You **talk about** the twist at the end (output) 📤\n",
    "\n",
    "---\n",
    "\n",
    "# ⚙️ Now, GRU — Gated Recurrent Unit\n",
    "\n",
    "## 👶 Even Simpler Analogy:\n",
    "\n",
    "GRU says:\n",
    "\n",
    "> “Why have 3 gates like LSTM? Let’s make it simpler with just **2 gates**.”\n",
    "\n",
    "| GRU Gate        | What It Does                     | Emoji |\n",
    "| --------------- | -------------------------------- | ----- |\n",
    "| **Update Gate** | Like input + forget combined     | 🔄    |\n",
    "| **Reset Gate**  | Controls how much past to forget | 🧹    |\n",
    "\n",
    "It **merges the forget and input gates** into **1 update gate** = faster and simpler!\n",
    "\n",
    "---\n",
    "\n",
    "## 🔣 GRU Equations:\n",
    "\n",
    "```python\n",
    "h_t = (1 - update) * old_memory + update * new_memory\n",
    "```\n",
    "\n",
    "### ✅ Fewer parameters = faster training\n",
    "\n",
    "### ❌ Slightly less flexible than LSTM in some cases\n",
    "\n",
    "---\n",
    "\n",
    "# 🥊 LSTM vs GRU — Side-by-Side\n",
    "\n",
    "| Feature        | LSTM                       | GRU                       |\n",
    "| -------------- | -------------------------- | ------------------------- |\n",
    "| Gates          | 3 (input, forget, output)  | 2 (update, reset)         |\n",
    "| Cell State     | Yes (separate from output) | No (combined with hidden) |\n",
    "| Complexity     | More complex               | Simpler                   |\n",
    "| Performance    | Better for long sequences  | Faster and still powerful |\n",
    "| Training Speed | Slower                     | Faster                    |\n",
    "\n",
    "---\n",
    "\n",
    "# 🧪 Code Examples (Keras)\n",
    "\n",
    "### 🔐 LSTM\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "### ⚙️ GRU\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(64, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 TL;DR Summary\n",
    "\n",
    "| Concept     | LSTM                                | GRU                       |\n",
    "| ----------- | ----------------------------------- | ------------------------- |\n",
    "| Memory Type | Has separate **cell state** 🧠      | Uses hidden state only    |\n",
    "| Gates       | 3 gates (input, forget, output)     | 2 gates (update, reset)   |\n",
    "| Size        | Bigger, more accurate on long tasks | Smaller, faster to train  |\n",
    "| Use Case    | Language models, translation        | Real-time chatbots, music |\n",
    "\n",
    "---\n",
    "\n",
    "# 🎓 Final Analogy:\n",
    "\n",
    "| Model | Like...                                              |\n",
    "| ----- | ---------------------------------------------------- |\n",
    "| RNN   | A person with **short-term memory** 🧏‍♂️            |\n",
    "| LSTM  | A person taking **notes with a smart notebook** 📓🧠 |\n",
    "| GRU   | A person with a **simple but fast brain** 🧠⚡        |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef3bac4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🧠 What is GRU? Gated Recurrent Unit\n",
    "\n",
    "---\n",
    "\n",
    "## 👶 Simple Definition:\n",
    "\n",
    "> GRU is a type of Recurrent Neural Network (RNN) that has a **smart memory system** built with just **two gates** — not three like LSTM.\n",
    "\n",
    "It's like a little brain that:\n",
    "\n",
    "* Remembers what’s important 🔁\n",
    "* Forgets what’s not 🧹\n",
    "\n",
    "And it does this faster than LSTM!\n",
    "\n",
    "---\n",
    "\n",
    "# 🤔 Why Was GRU Created?\n",
    "\n",
    "* LSTM was great but **a bit heavy and slow** 🐢\n",
    "* So GRU was designed to:\n",
    "\n",
    "  * Be **simpler** (fewer gates)\n",
    "  * Be **faster to train**\n",
    "  * Perform **almost as well** (or better in some tasks)\n",
    "\n",
    "---\n",
    "\n",
    "# 🧱 GRU Structure\n",
    "\n",
    "GRU has **2 gates** instead of 3:\n",
    "\n",
    "| Gate                | What It Does                           | Emoji |\n",
    "| ------------------- | -------------------------------------- | ----- |\n",
    "| **Update Gate** `z` | Decides **how much past info to keep** | 🔄    |\n",
    "| **Reset Gate** `r`  | Decides **how much past to forget**    | 🧹    |\n",
    "\n",
    "That's it. No output gate, no separate cell state — it's all handled in **1 hidden state**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ How GRU Works — Step by Step\n",
    "\n",
    "At each time step:\n",
    "\n",
    "1. 🧹 **Reset Gate (`r`)**:\n",
    "\n",
    "   * “Should I forget the old memory?”\n",
    "\n",
    "2. 🔄 **Update Gate (`z`)**:\n",
    "\n",
    "   * “Should I keep old memory or update with new info?”\n",
    "\n",
    "3. 🧠 **New Hidden State (`hₜ`)**:\n",
    "\n",
    "   * Combines the old and new info based on the gates' decisions\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 GRU Equations (Explained Simply)\n",
    "\n",
    "Let’s break it down:\n",
    "\n",
    "```python\n",
    "zₜ = σ(Wz · xₜ + Uz · hₜ₋₁)     # Update gate\n",
    "rₜ = σ(Wr · xₜ + Ur · hₜ₋₁)     # Reset gate\n",
    "\n",
    "h̃ₜ = tanh(W · xₜ + U · (rₜ * hₜ₋₁))  # New memory\n",
    "\n",
    "hₜ = (1 - zₜ) * hₜ₋₁ + zₜ * h̃ₜ       # Final output (mix old + new)\n",
    "```\n",
    "\n",
    "| Term   | Meaning                                       |\n",
    "| ------ | --------------------------------------------- |\n",
    "| `σ`    | Sigmoid activation (squashes between 0 and 1) |\n",
    "| `tanh` | Activation function for output                |\n",
    "| `xₜ`   | Input at time t                               |\n",
    "| `hₜ₋₁` | Previous hidden state                         |\n",
    "| `h̃ₜ`  | Candidate new hidden state                    |\n",
    "| `hₜ`   | Final hidden state/output at time t           |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Intuition:\n",
    "\n",
    "GRU says:\n",
    "\n",
    "> “Let me **control memory** using just 2 knobs — update and reset — and I’ll do the same job as LSTM faster!”\n",
    "\n",
    "---\n",
    "\n",
    "# 💻 GRU Code (in Keras)\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(64, input_shape=(timesteps, features)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 📊 GRU vs LSTM\n",
    "\n",
    "| Feature       | **GRU**                       | **LSTM**                          |\n",
    "| ------------- | ----------------------------- | --------------------------------- |\n",
    "| Gates         | 2 (update, reset)             | 3 (input, forget, output)         |\n",
    "| Simpler?      | ✅ Yes                         | ❌ More complex                    |\n",
    "| Memory        | Single hidden state           | Hidden + cell state               |\n",
    "| Speed         | ⚡ Faster                      | 🐢 Slower                         |\n",
    "| Performance   | Good in short & mid sequences | Better in long, complex sequences |\n",
    "| Training Time | Less                          | More                              |\n",
    "\n",
    "---\n",
    "\n",
    "# 💬 Real-Life Analogy\n",
    "\n",
    "Imagine a student reading a story.\n",
    "\n",
    "* GRU is a **fast learner**:\n",
    "\n",
    "  * Remembers important parts\n",
    "  * Erases what’s not needed\n",
    "  * All using **2 switches (update & reset)**\n",
    "\n",
    "Whereas LSTM is like:\n",
    "\n",
    "* A **note-taking overachiever** with 3 tools (input, forget, output)\n",
    "\n",
    "---\n",
    "\n",
    "# 🔁 When to Use GRU?\n",
    "\n",
    "| Use Case                    | GRU Works Well? |\n",
    "| --------------------------- | --------------- |\n",
    "| Real-time prediction        | ✅ (fast)        |\n",
    "| Text classification         | ✅               |\n",
    "| Speech/audio processing     | ✅               |\n",
    "| Time series forecasting     | ✅               |\n",
    "| Very long-term memory tasks | ❌ Use LSTM      |\n",
    "\n",
    "---\n",
    "\n",
    "# 🧠 TL;DR Summary\n",
    "\n",
    "| Term    | Definition                                     |\n",
    "| ------- | ---------------------------------------------- |\n",
    "| GRU     | Gated Recurrent Unit – a type of RNN           |\n",
    "| Goal    | Keep important memory, forget unimportant ones |\n",
    "| Gates   | Update gate 🔄, Reset gate 🧹                  |\n",
    "| Simpler | Yes, faster than LSTM                          |\n",
    "| Used in | NLP, time series, real-time tasks              |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
